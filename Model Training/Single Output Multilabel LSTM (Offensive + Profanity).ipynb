{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of7qzcrP3YVu"
      },
      "outputs": [],
      "source": [
        "colab=1\n",
        "if colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/MyDrive/Projects/NepSense/Prashant/Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHMIiN95pAZG"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install pycontractions\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install tensorflow==2.16.1\n",
        "!pip install numpy\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD9Ic8FrL_I_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Explicitly allow memory growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Your model building and training code should go here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JREBrpwpZ0B"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "# from Abiral.ipynb import csvwriter\n",
        "# import pycontractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhj4fnGcHusV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense,Flatten, GlobalMaxPooling1D,GlobalAveragePooling1D, Embedding, Conv1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.layers import Activation, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3x78iGBrAPZ"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "# wordnet is foro lemmanation and stemmation\n",
        "nltk.download(\"wordnet\")\n",
        "# punkt is for sent_tokenizer\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMbMwWkHLQtF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset=\"./Model Training Datas/embeddings.csv\"\n",
        "df = pd.read_csv(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwmqb9KXOV5K"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is your DataFrame with the 'SentimentScore' column\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# Create a countplot\n",
        "ax = sns.countplot(x='Label', data=df)\n",
        "\n",
        "# Add count annotations on top of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel(\"Sentiment Score\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"Sentiment Score Count\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5WxLI-OtMKA"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlSvVhzMazdc"
      },
      "outputs": [],
      "source": [
        "X=df[\"preprocessing_text\"]\n",
        "Y=df[\"Label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DKQEnfqK9Qi"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,stratify=Y, random_state=42)\n",
        "# The train set will be used to train our deep learning models\n",
        "# while test set will be used to evaluate how well our model performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tcRPkrUxhhr"
      },
      "outputs": [],
      "source": [
        "unique_labels = set(y_train)\n",
        "print(unique_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul-KoMGIbF22"
      },
      "outputs": [],
      "source": [
        "# print(y_train.count())\n",
        "# print(y_test.count())\n",
        "# X_train.head()\n",
        "# Create a countplot\n",
        "# Count occurrences of each value\n",
        "value_counts = np.bincount(y_train)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Counts of 1:\", value_counts[0])\n",
        "print(\"Counts of 2:\", value_counts[1])\n",
        "print(\"Counts of 3:\", value_counts[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBXTdnmEDL7Q"
      },
      "outputs": [],
      "source": [
        "type(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP5OOir8J1-4"
      },
      "outputs": [],
      "source": [
        "# Embedding layer expects the words to be in numeric form\n",
        "# Using Tokenizer function from keras.preprocessing.text library\n",
        "# Method fit_on_text trains the tokenizer\n",
        "# Method texts_to_sequences converts sentences to their numeric form\n",
        "import pickle\n",
        "#Must run above shell\n",
        "single_output_mutlilabel_tokenizer =  Tokenizer(num_words=90000,\n",
        "                      lower=True,\n",
        "                      split=' ',\n",
        "                      char_level=False,\n",
        "                      oov_token='<UNK>',\n",
        "                      document_count=0)\n",
        "\n",
        "single_output_mutlilabel_tokenizer.fit_on_texts(X_train)\n",
        "# word_tokenizer.fit_on_texts(y_train)\n",
        "\n",
        "# Save the tokenizer to a file on Google Drive\n",
        "# tokenizer_path = '/content/drive/MyDrive/CommentSense/Code/Prashant/ABIRAL COPY/tokenizer.pkl'\n",
        "# tokenizer_path = '/content/drive/My Drive/CommentSenseData/newtokenizer.pkl'\n",
        "\n",
        "# with open(tokenizer_path, 'wb') as tokenizer_file:\n",
        "#     pickle.dump(word_tokenizer, tokenizer_file)\n",
        "\n",
        "wordindex=single_output_mutlilabel_tokenizer.word_index\n",
        "X_train = single_output_mutlilabel_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = single_output_mutlilabel_tokenizer.texts_to_sequences(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRJbd6CwLPj8"
      },
      "outputs": [],
      "source": [
        "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
        "\n",
        "vocab_length = len(wordindex) + 1\n",
        "\n",
        "vocab_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxCQ3XeWMKTv"
      },
      "outputs": [],
      "source": [
        "# Padding all reviews to fixed length 100\n",
        "max_len=500\n",
        "X_train = pad_sequences(X_train, padding='post',maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, padding='post',maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-112irhZJ4W"
      },
      "outputs": [],
      "source": [
        "#print(test_padding[0])\n",
        "#train_padding[0]\n",
        "X_train.shape\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTlCY7NeuHE"
      },
      "outputs": [],
      "source": [
        "# y_train=to_categorical(y_train.values,num_classes=3)\n",
        "# y_test=to_categorical(y_test.values,num_classes=3)\n",
        "# y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHM2wTGoWxcY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Create an instance of OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Convert pandas Series to NumPy array\n",
        "y_train_array = np.array(y_train).reshape(-1, 1)\n",
        "y_test_array = np.array(y_test).reshape(-1, 1)\n",
        "\n",
        "# Fit and transform on training data\n",
        "y_train = encoder.fit_transform(y_train_array)\n",
        "\n",
        "# Transform only on test data\n",
        "y_test = encoder.transform(y_test_array)\n",
        "\n",
        "# Check the shape of the transformed arrays\n",
        "print(\"Shape of y_train_encoded:\", y_train.shape)\n",
        "print(\"Shape of y_test_encoded:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ4P0-mAgcuK"
      },
      "outputs": [],
      "source": [
        "# Neural Network architecture\n",
        "from keras.constraints import max_norm\n",
        "import tensorflow\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "regularise = tensorflow.keras.regularizers.l2(0.0001)\n",
        "single_output_mutlilabel_lstm_model = Sequential()\n",
        "embedding_layer =Embedding(vocab_length,256,input_length=X_train.shape[1])\n",
        "single_output_mutlilabel_lstm_model.add(embedding_layer)\n",
        "single_output_mutlilabel_lstm_model.add(Dropout(0.7))\n",
        "single_output_mutlilabel_lstm_model.add(Bidirectional(LSTM(256,return_sequences=True))),\n",
        "single_output_mutlilabel_lstm_model.add(Bidirectional(LSTM(128,kernel_constraint=max_norm(3))))\n",
        "single_output_mutlilabel_lstm_model.add(Dense(128,activation='relu',kernel_regularizer=regularise))\n",
        "single_output_mutlilabel_lstm_model.add(Dropout(0.7))\n",
        "single_output_mutlilabel_lstm_model.add(Dense(3,activation='softmax'))\n",
        "\n",
        "# Model compiling\n",
        "optimizer = Adam(learning_rate=0.00005)\n",
        "single_output_mutlilabel_lstm_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(single_output_mutlilabel_lstm_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sl1igUJ1yfT"
      },
      "outputs": [],
      "source": [
        "y_train=np.array(y_train.toarray())\n",
        "X_train=np.array(X_train)\n",
        "y_test=np.array(y_test.toarray())\n",
        "X_test=np.array(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE1M1MbCLQtH"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * 0.2\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xBvGdFHDBYQ"
      },
      "outputs": [],
      "source": [
        "# lstm_model.build(input_shape=(batch_size, sequence_length, input_dim))\n",
        "# plot_model(lstm_model, to_file='model_diagram.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtFrvJA7kGXW"
      },
      "outputs": [],
      "source": [
        "from keras.models import save_model\n",
        "import json\n",
        "# Train the model\n",
        "epochs =30\n",
        "batch_size = 150\n",
        "# modeltrain=lstm_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=False)\n",
        "# lr_scheduler = LearningRateScheduler(scheduler)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0000001)\n",
        "regularizemodel=single_output_mutlilabel_lstm_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[early_stopping,lr_scheduler])\n",
        "\n",
        "\n",
        "# # Assuming you have a Keras model named 'model'\n",
        "# lstm_model.save('/content/drive/My Drive/CommentSenseData/LSTMsentimentmodel.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOvzM1aBozHO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Function to plot training and validation accuracy\n",
        "def plot_accuracy(history):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('MultiLabel Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot training and validation loss\n",
        "def plot_loss(history):\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('MultiLabel Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "  # Plot training and validation accuracy\n",
        "plot_accuracy(regularizemodel)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plot_loss(regularizemodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOLdzGXL6Vky"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "score = single_output_mutlilabel_lstm_model.evaluate(X_test,y_test,batch_size=batch_size)\n",
        "print(\"Testing Accuracy(%): \", score[1]*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOublOStrMGc"
      },
      "outputs": [],
      "source": [
        "y_predictions = single_output_mutlilabel_lstm_model.predict(X_test)\n",
        "y_pred_labels = np.array([ np.argmax(i) for i in y_predictions])\n",
        "y_test_labels = np.array([ np.argmax(i) for i in y_test])\n",
        "\n",
        "\n",
        "# print(y_predictions)\n",
        "# print(y_pred_labels)\n",
        "# print(y_test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97HW6Ut7s_A2"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "labels=['None', 'Offensive','Profane']\n",
        "plt.figure(figsize=(4,4))\n",
        "sn.heatmap(confusion,  xticklabels=labels, yticklabels=labels, fmt='d', annot=True, cmap=plt.cm.Blues)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqHAhSdutBOu"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('\\nClassification Report for Bidirectional LSTM\\n')\n",
        "print(classification_report(y_test_labels, y_pred_labels, target_names=['Class  None',\t 'Class Offensive','Class Profane ']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTiATMPt4G89"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "single_output_mutlilabel_model_name= 'Mutlilabel_LSTM_Offensive_Profane'\n",
        "single_output_mutlilabel_model_path= f'/content/drive/MyDrive/Projects/NepSense/{single_output_mutlilabel_model_name}.h5'\n",
        "single_output_mutlilabel_tokenizer_path= f'/content/drive/MyDrive/Projects/NepSense/{single_output_mutlilabel_model_name}.pkl'\n",
        "\n",
        "single_output_mutlilabel_lstm_model.save(single_output_mutlilabel_model_path)\n",
        "# Save the tokenizer\n",
        "with open(single_output_mutlilabel_tokenizer_path, 'wb') as f:\n",
        "    pickle.dump(single_output_mutlilabel_tokenizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ShZdZG7_r9Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "\n",
        "single_output_mutlilabel_lstm_model = tf.keras.models.load_model(single_output_mutlilabel_model_path)\n",
        "\n",
        "with open(single_output_mutlilabel_tokenizer_path, 'rb') as f:\n",
        "    single_output_mutlilabel_tokenizer = pickle.load(f)\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    sequences = single_output_mutlilabel_tokenizer.texts_to_sequences([text])\n",
        "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=500)\n",
        "    return padded_sequences\n",
        "\n",
        "# Function to predict the sentiment\n",
        "def predict_sentiment(text):\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    prediction = single_output_mutlilabel_lstm_model.predict(preprocessed_text)\n",
        "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
        "    labels = ['None', 'Offensive', 'Profane']\n",
        "    return labels[predicted_label]\n",
        "\n",
        "# Ask the user for input text\n",
        "user_input = input(\"Please enter a text: \")\n",
        "\n",
        "# Predict the sentiment\n",
        "result = predict_sentiment(user_input)\n",
        "\n",
        "# Print the result\n",
        "print(f\"The predicted sentiment is: {result}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mezUWRVDSMKG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
