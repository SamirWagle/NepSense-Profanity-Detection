{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas==2.2.2\n",
    "# %pip install numpy==1.26.4\n",
    "# %pip install nltk==3.8.1\n",
    "# %pip install pycontractions\n",
    "# %pip install scikit-learn\n",
    "# %pip install tqdm\n",
    "# %pip install tensorflow=2.16.1\n",
    "# %pip install numpy==1.26.4\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install langdetect\n",
    "# %pip install prettytable==3.11.0\n",
    "# %pip install keras==3.3.3\n",
    "# %pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Loading the required file and model from google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Download:\n",
    "    \"\"\"This class helps to download files from Google Drive\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def download_file_from_google_drive(self, file_id, destination):\n",
    "        \"\"\"Download file from Google Drive.\n",
    "        \n",
    "        Arguments:\n",
    "            file_id {string} -- Unique ID of the file in Google Drive.\n",
    "            destination {string} -- Destination path where the file will be saved.\n",
    "        \"\"\"\n",
    "        def get_confirm_token(response):\n",
    "            \"\"\"Retrieve confirmation token from the response cookies.\n",
    "            \n",
    "            Arguments:\n",
    "                response {requests.Response} -- Response object from the initial request.\n",
    "                \n",
    "            Returns:\n",
    "                token {string} -- Confirmation token if present, otherwise None.\n",
    "            \"\"\"\n",
    "            for key, value in response.cookies.items():\n",
    "                if key.startswith('download_warning'):\n",
    "                    return value\n",
    "            return None\n",
    "\n",
    "        def save_response_content(response, destination):\n",
    "            \"\"\"Save the content of the response to a file.\n",
    "            \n",
    "            Arguments:\n",
    "                response {requests.Response} -- Response object containing the file content.\n",
    "                destination {string} -- Destination path where the file will be saved.\n",
    "            \"\"\"\n",
    "            CHUNK_SIZE = 32768  # Define chunk size for streaming download\n",
    "            total_size = int(response.headers.get('content-length', 0))  # Get total file size from headers\n",
    "            with open(destination, \"wb\") as f, tqdm(\n",
    "                desc=destination,\n",
    "                total=total_size,\n",
    "                unit='B',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                    if chunk:  # Filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "                        bar.update(len(chunk))\n",
    "\n",
    "        URL = \"https://docs.google.com/uc?export=download\"  # Google Drive download URL\n",
    "        session = requests.Session()  # Create a session object\n",
    "        response = session.get(URL, params={'id': file_id}, stream=True)  # Initial request to get the file\n",
    "        token = get_confirm_token(response)  # Check for confirmation token\n",
    "\n",
    "        if token:\n",
    "            # If confirmation token exists, make another request with the token\n",
    "            params = {'id': file_id, 'confirm': token}\n",
    "            response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "        # Save the content of the response to the specified destination\n",
    "        save_response_content(response, destination)\n",
    "\n",
    "# Instantiate the Download class\n",
    "downloader = Download()\n",
    "\n",
    "# Dictionary containing filenames and their corresponding Google Drive file IDs\n",
    "file_ids = {\n",
    "    'Multi_Model_Multi_Output.h5': '1L8V2by-UzjxPSL7NyZckTkarSt8AHGBw',\n",
    "    'Multi_Model_Multi_Output.pkl': '1P5_Nz52s_Wokuymp26aPOp9GgOe8nuXF',\n",
    "    'Mutlilabel_LSTM_Offensive_Profane.h5': '1bwOOub0XeazLwXisQ981EDMXuvsQvHhE',\n",
    "    'Mutlilabel_LSTM_Offensive_Profane.pkl': '18xonXzSUxkpE1NS4dJnZR37uySvNG2B6',\n",
    "    'Binomial_LSTM_Profane.h5': '1dr87w_4iWdiV2EUOGi4asHF1RNtWeS4f',\n",
    "    'Binomial_LSTM_Profane.pkl': '1kJTe25qdBu6q5i6Gk5VYtDAA06Puj9Ml',\n",
    "    'Binomial_LSTM_Offensive.keras': '1rDBl9_WvaA7YWNx_sZ08BQFGTTKCk5wS',\n",
    "    'Binomial_LSTM_Offensive.pkl': '1UdLUoalPqotM5tYHwYwZDXffp0mbnLZc',\n",
    "    \"Emoji Sheets - Emoji Only.csv\":\"1bHK-ofASD0XC-Z1gtbKbalWj6B_71umw\"\n",
    "}\n",
    "\n",
    "# Specify your download directory\n",
    "download_dir = './downloaded_files/'\n",
    "\n",
    "# Ensure the download directory exists\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Download each file\n",
    "for filename, file_id in file_ids.items():\n",
    "    try:\n",
    "        if os.path.exists(os.path.join(download_dir, filename)):\n",
    "            # Check if file already exists to avoid re-downloading\n",
    "            print(f\"File {filename} already downloaded\")\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    except FileNotFoundError:\n",
    "        # Download the file if it does not exist\n",
    "        print(f\"Downloading {filename} : https://drive.google.com/file/d/{file_id}\")\n",
    "        destination_path = os.path.join(download_dir, filename)\n",
    "        downloader.download_file_from_google_drive(file_id, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BertTokenizer\n",
    "from nltk.util import ngrams\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Example usage to download your models and tokenizers\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # After downloading, load your models and tokenizers\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "\n",
    "    # Load Multi Model Multi Output\n",
    "    # Load the Keras model from the specified file\n",
    "    multi_model = tf.keras.models.load_model(os.path.join(download_dir, 'Multi_Model_Multi_Output.h5'))\n",
    "    # Load the tokenizer using pickle\n",
    "    with open(os.path.join(download_dir, 'Multi_Model_Multi_Output.pkl'), 'rb') as f:\n",
    "        multi_tokenizer = pickle.load(f)\n",
    "\n",
    "    # Load Multilabel LSTM Offensive Profane\n",
    "    # Load the Keras model for multilabel offensive and profane text classification\n",
    "    multilabel_lstm_model = tf.keras.models.load_model(os.path.join(download_dir, 'Mutlilabel_LSTM_Offensive_Profane.h5'))\n",
    "    # Load the tokenizer for the multilabel model\n",
    "    with open(os.path.join(download_dir, 'Mutlilabel_LSTM_Offensive_Profane.pkl'), 'rb') as f:\n",
    "        multilabel_tokenizer = pickle.load(f)\n",
    "\n",
    "    # Load Binomial LSTM Profane\n",
    "    # Load the Keras model for binomial classification of profane text\n",
    "    binomial_lstm_model_profanity = load_model(os.path.join(download_dir, 'Binomial_LSTM_Profane.h5'))\n",
    "    # Load the tokenizer for the binomial model\n",
    "    with open(os.path.join(download_dir, 'Binomial_LSTM_Profane.pkl'), 'rb') as f:\n",
    "        binomial_tokenizer_profanity = pickle.load(f)\n",
    "\n",
    "    # Use the same h5 file for another model\n",
    "    # Load the Keras model for binomial classification of offensive text\n",
    "    binomial_lstm_model_offensive = tf.keras.models.load_model(os.path.join(download_dir, 'Binomial_LSTM_Offensive.keras'))\n",
    "    # Load the tokenizer for the binomial offensive model\n",
    "    with open(os.path.join(download_dir, 'Binomial_LSTM_Offensive.pkl'), 'rb') as f:\n",
    "        binomial_tokenizer_offensive = pickle.load(f)\n",
    "\n",
    "    # Print a message to indicate successful loading of models and tokenizers\n",
    "    print(\"Models and tokenizers loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the emoji files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file containing the list of emojis\n",
    "emoji_df = pd.read_csv(os.path.join(download_dir, 'Emoji Sheets - Emoji Only.csv'))\n",
    "\n",
    "# Extract the emojis into a list from the DataFrame\n",
    "emoji_list = emoji_df['Emoji_List'].tolist()\n",
    "\n",
    "# Start building the regular expression pattern for emojis\n",
    "pattern = '['\n",
    "\n",
    "# Append each emoji code point to the pattern string, ensuring each one is 8 digits\n",
    "for cp in emoji_list:\n",
    "    pattern += f'\\\\U{cp[1:]:0>8}'\n",
    "\n",
    "# Close the pattern string\n",
    "pattern += ']'\n",
    "\n",
    "# Compile the regular expression to match emojis\n",
    "emoji_pattern = re.compile(pattern, re.UNICODE)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from the given text using the compiled emoji pattern.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text from which emojis need to be removed.\n",
    "        \n",
    "    Returns:\n",
    "        str -- Text with emojis removed.\n",
    "    \"\"\"\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import BertTokenizer\n",
    "from nltk.util import ngrams\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# Initialize BERT model and tokenizer\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(model_name)\n",
    "model_bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_bert.to(device)  # Move model to the specified device\n",
    "model_bert.eval()  # Set model to evaluation mode\n",
    "\n",
    "def get_ngram_embeddings(text, n):\n",
    "    \"\"\"Generate embeddings for n-grams of the given text using BERT.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text to be tokenized and embedded.\n",
    "        n {int} -- The 'n' in n-grams.\n",
    "        \n",
    "    Returns:\n",
    "        np.array -- Array of n-gram embeddings.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer_bert.encode(text, add_special_tokens=True)\n",
    "    text_ngrams = list(ngrams(tokenized_text, n))\n",
    "    embeddings = []\n",
    "\n",
    "    for gram in text_ngrams:\n",
    "        input_ids = torch.tensor(gram).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert(input_ids)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "        embeddings.append(sentence_embedding)\n",
    "\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    return embeddings_array\n",
    "\n",
    "def predict_labels(text):\n",
    "    \"\"\"Predict labels using a multi-output model.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        tuple -- Gender label, profanity label, prediction probabilities, and prediction accuracy.\n",
    "    \"\"\"\n",
    "    n = 2  # Define the 'n' for n-grams\n",
    "    embeddings = get_ngram_embeddings(text, n)\n",
    "    max_len = multi_model.input_shape[1]  # Get the maximum input length for the model\n",
    "    padded_embeddings = pad_sequences([embeddings], maxlen=max_len, padding='post', dtype='float32')\n",
    "    predictions = multi_model.predict(padded_embeddings)\n",
    "    gender_pred = predictions[0]\n",
    "    profanity_pred = predictions[1]\n",
    "    gender_label = (gender_pred > 0.5).astype(int).flatten()[0]\n",
    "    profanity_label = np.argmax(profanity_pred, axis=1)[0]\n",
    "    pred_accuracy = predictions[1][0][profanity_label]\n",
    "    return gender_label, profanity_label, predictions, pred_accuracy\n",
    "\n",
    "def preprocess_text(text, tokenizer):\n",
    "    \"\"\"Preprocess text for model input.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text to be tokenized and padded.\n",
    "        tokenizer {Tokenizer} -- Tokenizer to convert text to sequences.\n",
    "        \n",
    "    Returns:\n",
    "        np.array -- Padded sequences of the input text.\n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequences = pad_sequences(sequences, padding='post', maxlen=500)\n",
    "    return padded_sequences\n",
    "\n",
    "def predict_multilabel(text):\n",
    "    \"\"\"Predict labels using a multilabel model.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        tuple -- Profanity label, prediction probabilities, and prediction accuracy.\n",
    "    \"\"\"\n",
    "    preprocessed_text = preprocess_text(text, multilabel_tokenizer)\n",
    "    prediction = multilabel_lstm_model.predict(preprocessed_text)\n",
    "    profanity_pred = np.argmax(prediction, axis=1)[0]\n",
    "    pred_accuracy = prediction[0][profanity_pred]\n",
    "    return profanity_pred, prediction, pred_accuracy\n",
    "\n",
    "def predict_binomial(text, model, tokenizer):\n",
    "    \"\"\"Predict binary labels using a binomial model.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text for prediction.\n",
    "        model {Model} -- Binomial classification model.\n",
    "        tokenizer {Tokenizer} -- Tokenizer to convert text to sequences.\n",
    "        \n",
    "    Returns:\n",
    "        tuple -- Predicted label and prediction accuracy.\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=500, padding='post')\n",
    "    prediction = model.predict(input_padded)\n",
    "    predicted_label = prediction.argmax(axis=-1)[0]\n",
    "    pred_accuracy = prediction[0][predicted_label]\n",
    "    return predicted_label, pred_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English to Nepali Transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "from ai4bharat.transliteration import XlitEngine\n",
    "\n",
    "# Initialize the transliteration engine for Nepali with beam width 10\n",
    "e = XlitEngine([\"ne\"], beam_width=10, src_script_type=\"en\")\n",
    "\n",
    "def nepali_nlp_text_conversion(text):\n",
    "    \"\"\"Convert the given text to Nepali using transliteration if it is not already in Nepali.\n",
    "    \n",
    "    Arguments:\n",
    "        text {str} -- Input text to be converted.\n",
    "        \n",
    "    Returns:\n",
    "        str -- Converted text if applicable, otherwise the original text.\n",
    "    \"\"\"\n",
    "    # Check if the input is a non-empty string\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        try:\n",
    "            # Detect the language of the input text\n",
    "            lang = detect(text)\n",
    "            if lang != \"ne\":\n",
    "                # If the detected language is not Nepali, perform transliteration\n",
    "                temp_results = e.translit_sentence(text)[\"ne\"]\n",
    "                return temp_results\n",
    "            # If the detected language is Nepali, return the original text\n",
    "        except LangDetectException:\n",
    "            # Handle language detection failure\n",
    "            print(\"Failed to detect language\")\n",
    "            pass\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gettting the input from user and print the result in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take text input from the user\n",
    "user_text = input(\"Enter text: \")\n",
    "\n",
    "# Convert Nepali text, remove emojis\n",
    "user_text = nepali_nlp_text_conversion(remove_emojis(user_text))\n",
    "\n",
    "# Predict using the multi-output model\n",
    "gender_label, multi_profanity_label, multi_predictions, multi_pred_accuracy = predict_labels(user_text)\n",
    "# Determine gender from label\n",
    "gender = 'Male' if gender_label == 1 else 'Female'\n",
    "# Define profanity classes\n",
    "multi_profanity_classes = {0: 'Non-Offensive', 1: 'Offensive', 2: 'Profane'}\n",
    "# Get profanity class from label\n",
    "multi_profanity = multi_profanity_classes[multi_profanity_label]\n",
    "\n",
    "# Predict using the multilabel LSTM model\n",
    "multilabel_profanity_label, multilabel_predictions, multilabel_pred_accuracy = predict_multilabel(user_text)\n",
    "# Define multilabel classes\n",
    "multilabel_classes = ['Non-Offensive', 'Offensive', 'Profane']\n",
    "# Get multilabel profanity class from label\n",
    "multilabel_profanity = multilabel_classes[multilabel_profanity_label]\n",
    "\n",
    "# Predict using the binomial LSTM models\n",
    "binomial_profanity, binomial_profanity_pred_accuracy = predict_binomial(user_text, binomial_lstm_model_profanity, binomial_tokenizer_profanity)\n",
    "# Determine profanity from binomial model\n",
    "binomial_profanity = 'Profane' if binomial_profanity == 1 else 'Non-Profane'\n",
    "\n",
    "binomial_offensive, binomial_offensive_pred_accuracy = predict_binomial(user_text, binomial_lstm_model_offensive, binomial_tokenizer_offensive)\n",
    "# Determine offensiveness from binomial model\n",
    "binomial_offensive = 'Offensive' if binomial_offensive == 1 else 'Non-Offensive'\n",
    "\n",
    "# Create the additional information table with the required format\n",
    "additional_info_table = PrettyTable()\n",
    "additional_info_table.field_names = [\"--------Test Result Of--------\"]\n",
    "# Add the user's text to the table\n",
    "additional_info_table.add_row([f\"                              {user_text}                            \"])\n",
    "\n",
    "# Display the results in a table\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"Gender\", \"Profanity/Offensiveness\", \"Pred Accuracy\"]\n",
    "table.add_row([\"Multi-Output Model\", gender, multi_profanity, multi_pred_accuracy])\n",
    "table.add_row([\"Multilabel LSTM Model\", \"-\", multilabel_profanity, multilabel_pred_accuracy])\n",
    "table.add_row([\"Binomial LSTM Profanity Model\", \"-\", binomial_profanity, binomial_profanity_pred_accuracy])\n",
    "table.add_row([\"Binomial LSTM Offensive Model\", \"-\", binomial_offensive, binomial_offensive_pred_accuracy])\n",
    "\n",
    "# Print the tables\n",
    "print(additional_info_table)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(additional_info_table)\n",
    "print(table)\n",
    "\n",
    "# Ask the user to verify the results\n",
    "print(\"Verify the results:\")\n",
    "\n",
    "# Create a dictionary to store the user's responses\n",
    "user_responses = {}\n",
    "\n",
    "# List of models for user verification\n",
    "model_list = [\"Multi-Output Model\", \"Multilabel LSTM Model\", \"Binomial LSTM Profanity Model\", \"Binomial LSTM Offensive Model\"]\n",
    "\n",
    "# Get the user's responses\n",
    "for i, model in enumerate(model_list, start=1):\n",
    "    response = input(f\"Was the result correct for {model}? (1 for yes/0 for no): \")\n",
    "    user_responses[model] = response\n",
    "\n",
    "# Print the user's responses\n",
    "print(\"\\nUser Responses about the correctness of model:\")\n",
    "for model, response in user_responses.items():\n",
    "    print(f\"{model}: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the verified result in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the file name\n",
    "tested_file_name = 'testeddata.xlsx'\n",
    "\n",
    "# Try to import the Excel file\n",
    "try:\n",
    "    if os.path.exists(tested_file_name):\n",
    "        # Read the existing Excel file into a DataFrame\n",
    "        tested_df = pd.read_excel(tested_file_name,index_col=False)\n",
    "        print(\"Excel file imported successfully.\")\n",
    "    else:\n",
    "        # If the file doesn't exist, raise FileNotFoundError\n",
    "        raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    # Create a new DataFrame with specified columns if the file is not found\n",
    "    tested_df = pd.DataFrame(columns=[\n",
    "        'user_text', 'gender', 'multi_profanity', 'multi_pred_accuracy', 'multi_profanity_correctness',\n",
    "        'multilabel_profanity', 'multilabel_pred_accuracy', 'multilabel_profanity_correctness',\n",
    "        'binomial_profanity', 'binomial_profanity_pred_accuracy', 'binomial_profanity_correctness',\n",
    "        'binomial_offensive', 'binomial_offensive_pred_accuracy', 'binomial_offensive_correctness'\n",
    "    ])\n",
    "    print(\"Excel file not found. New DataFrame created.\")\n",
    "    \n",
    "# Print the DataFrame\n",
    "tested_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the data\n",
    "new_row = {\n",
    "    'user_text': [user_text],\n",
    "    'gender': [gender],\n",
    "    'multi_profanity': [multi_profanity],\n",
    "    'multi_pred_accuracy': [multi_pred_accuracy],\n",
    "    'multi_profanity_correctness': [user_responses[model_list[0]]],\n",
    "    'multilabel_profanity': [multilabel_profanity],\n",
    "    'multilabel_pred_accuracy': [multilabel_pred_accuracy],\n",
    "    'multilabel_profanity_correctness': [user_responses[model_list[1]]],\n",
    "    'binomial_profanity': [binomial_profanity],\n",
    "    'binomial_profanity_pred_accuracy': [binomial_profanity_pred_accuracy],\n",
    "    'binomial_profanity_correctness': [user_responses[model_list[2]]],\n",
    "    'binomial_offensive': [binomial_offensive],\n",
    "    'binomial_offensive_pred_accuracy': [binomial_offensive_pred_accuracy],\n",
    "    'binomial_offensive_correctness': [user_responses[model_list[3]]]\n",
    "}\n",
    "\n",
    "# Create a new DataFrame\n",
    "new_row = pd.DataFrame(new_row)\n",
    "\n",
    "\n",
    "# Assign the new row to the DataFrame\n",
    "tested_df.loc[len(tested_df)] = new_row.iloc[0].values\n",
    "tested_df.to_excel(tested_file_name,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
